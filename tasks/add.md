# Inbox

2026-02-27 14:41:20,320 - asky.core.api_client - DEBUG - {'status_code': 429, 'url': 'https://openrouter.ai/api/v1/chat/completions', 'headers': {'Date': 'Fri, 27 Feb 2026 13:41:20 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Access-Control-Allow-Origin': '*', 'Permissions-Policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'Referrer-Policy': 'no-referrer, strict-origin-when-cross-origin', 'X-Content-Type-Options': 'nosniff', 'Server': 'cloudflare', 'CF-RAY': '9d4819603dc8910e-AMS'}, 'content_type': 'application/json', 'body': {'error': {'message': 'Provider returned error', 'code': 429, 'metadata': {'raw': 'google/gemini-2.5-flash-lite is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations', 'provider_name': 'Google AI Studio', 'is_byok': False}}, 'user_id': 'user_2pH42wmP5chSEClytsj6aZJGODu'}}

I want to refactor our main summarization image interface all model definition structure to list of models instead of one model per category. I want to be able to let the users to select more than one models if they like. So if we hit an issue with the first option we can try the second option or in the future we might allow some workflow that balances distributes the calls to all the selected models in order or randomly, but that's not in scope now. I just want allowing defining more than one model per category and a fallback mechanism for certain type of errors and about that certain type errors. In a particular case with the open router I hit an upstream rate limiting issue. In that case the problem wasn't about wasn't related with us. So backing off for a while was not helpful. In that case we should have just switched to second model. Actually in any case in the case of rate limiting we can do that but we still should catch upstream problems with a router style providers like open router and properly communicate this so users will not be confused about why they get rate limiting for a very small number of queries they are making at the moment.