# --- Research Mode Settings ---
# Deep research mode with RAG-based content retrieval
[research]
# Enable research mode tools
enabled = true

# Restrict local-source ingestion to these root directories.
# Local targets in prompts are treated as corpus-relative paths under these roots.
# Leave empty to disable builtin local filesystem ingestion.
local_document_roots = []

# Cache TTL in hours (cached pages expire after this time)
cache_ttl_hours = 24

# Evidence extraction (post-retrieval LLM fact extraction)
evidence_extraction_enabled = false
evidence_extraction_max_chunks = 10

# Maximum links to return per URL (before relevance filtering)
max_links_per_url = 50

# Maximum links after relevance filtering (when query is provided)
max_relevant_links = 20

# Chunk size for content splitting (tokens, clamped by embedding model max length)
chunk_size = 256

# Chunk overlap for context continuity (tokens)
chunk_overlap = 48

# Number of relevant chunks to retrieve per URL in RAG queries
max_chunks_per_retrieval = 5

# Background summarization thread pool size
summarization_workers = 2

# Maximum findings to return from research memory queries
memory_max_results = 10

[research.chromadb]
# Directory used by ChromaDB's persistent client.
persist_directory = "~/.config/asky/chromadb"

# Collection names for research mode vectors.
chunks_collection = "asky_content_chunks"
links_collection = "asky_link_embeddings"
findings_collection = "asky_research_findings"

# Shared pre-LLM source shortlisting configuration
[research.source_shortlist]
# Master switch for source shortlisting before first LLM call
enabled = true

# Enable source shortlisting in research mode
enable_research_mode = true

# Enable source shortlisting in standard (non-research) mode
enable_standard_mode = true

# If true, run web search even when the prompt already includes URLs
search_with_seed_urls = false

# If true, shortlist expansion extracts and adds links from seed URLs.
seed_link_expansion_enabled = true

# Max number of seed pages to expand for links.
seed_link_max_pages = 3

# Max extracted links to consider from each seed page before dedupe/caps.
seed_links_per_page = 50

# Number of search results to pull as initial candidates
search_result_count = 40

# Candidate caps to control pre-LLM fetch cost
max_candidates = 40
max_fetch_urls = 20

# Number of ranked candidates to pass forward
top_k = 8

# Extracted text thresholds and payload sizing
min_content_chars = 300
max_scoring_chars = 5000
snippet_chars = 700
doc_lead_chars = 1400
query_fallback_chars = 600

# Keyphrase extraction controls
keyphrase_min_query_chars = 220
keyphrase_top_k = 20
search_phrase_count = 5

# Scoring heuristics
short_text_threshold = 700
same_domain_bonus = 0.05
overlap_bonus_weight = 0.10
short_text_penalty = 0.10
noise_path_penalty = 0.15

# Corpus-aware shortlisting: use ingested document content to build better queries
corpus_lead_chars = 4000
corpus_max_keyphrases = 15
corpus_max_query_titles = 3

# Query expansion before shortlist and local-source ingestion
query_expansion_enabled = true
query_expansion_mode = "deterministic" # or "llm"
max_sub_queries = 4

# Local sentence-transformers settings
[research.embedding]
# Model name to load in-process
model = "all-MiniLM-L6-v2"

# Batch size for local encoding
batch_size = 32

# Torch device for embedding model (cpu, cuda, mps)
device = "cpu"

# Normalize vectors so cosine similarity is stable across retrieval calls
normalize = true

# If true, do not download models and use only local cache.
# If false, models are downloaded from Hugging Face automatically when missing.
local_files_only = false
