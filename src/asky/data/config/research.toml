# --- Research Mode Settings ---
# Deep research mode with RAG-based content retrieval
[research]
# Enable research mode tools
enabled = true

# Cache TTL in hours (cached pages expire after this time)
cache_ttl_hours = 24

# Maximum links to return per URL (before relevance filtering)
max_links_per_url = 50

# Maximum links after relevance filtering (when query is provided)
max_relevant_links = 20

# Chunk size for content splitting (tokens, clamped by embedding model max length)
chunk_size = 256

# Chunk overlap for context continuity (tokens)
chunk_overlap = 48

# Number of relevant chunks to retrieve per URL in RAG queries
max_chunks_per_retrieval = 5

# Background summarization thread pool size
summarization_workers = 2

# Maximum findings to return from research memory queries
memory_max_results = 10

# Optional adapter layer to route research targets (e.g. local://...) through
# user-defined custom tools. This keeps research tools generic while letting
# you plug in your own readers/indexers (PDF, docs, local files, etc).
#
# Contract for adapter tool stdout (JSON):
# {
#   "title": "Source title",
#   "content": "Plain text content (optional for directory-like targets)",
#   "links": [{"text": "Doc title", "href": "local://doc-id"}],
#   "error": null
# }
#
# [research.source_adapters.local]
# enabled = false
# prefix = "local://"
# tool = "local_research_source" # Optional single-tool mode (used for both discover/read)
# discover_tool = "local_list_content" # Optional override for extract_links-style discovery
# read_tool = "local_read_content"     # Optional override for full/relevant content retrieval

[research.chromadb]
# Directory used by ChromaDB's persistent client.
persist_directory = "~/.config/asky/chromadb"

# Collection names for research mode vectors.
chunks_collection = "asky_content_chunks"
links_collection = "asky_link_embeddings"
findings_collection = "asky_research_findings"

# Local sentence-transformers settings
[research.embedding]
# Model name to load in-process
model = "all-MiniLM-L6-v2"

# Batch size for local encoding
batch_size = 32

# Torch device for embedding model (cpu, cuda, mps)
device = "cpu"

# Normalize vectors so cosine similarity is stable across retrieval calls
normalize = true

# If true, do not download models and use only local cache
local_files_only = false
