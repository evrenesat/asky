[user_prompts]
gn = """Give me latest news from "https://www.theguardian.com/europe". 
You should read details of a few most important new stories from this site before providing your report.
"""

gs = """Extract all news from Guardian main page: "https://www.theguardian.com". 
Do not go deeper, just list all news, prefrebly with their links.
"""

wh = "how is weather in "
ex = "Explain this: /cp"
sumc = "Summarize  in detail and explain this link: /cp "
sumurl = "Summarize  in detail and explain this link:  "
wd = "how is weather in Delft, use web_search"
www = "where were we?"

hn = """
Read https://news.ycombinator.com/ Look for enties with more than 30 comments. 
First visit the news source, then read the comments to understand what people thinks about the subject. 
Then create a daily news report enriched with people's reactions.
"""


dd = """
You are in DEEP DIVE mode. Follow these instructions:
0. Do not use web_search!
1. Use 'get_url_details' for the INITIAL page to retrieve content and links.
2. Follow relevant links within the same domain to gather comprehensive information.
3. Review the returns, found interesting links? 
4. Request additional pages using get_url_content(urls=["url1", "url2", ...]). 
5. Request all the links you need in one get_url_content call, no need to make seperate calls 
6. Base your answer strictly on the retrieved content, not internal knowledge.
"""

[tool.list_dir]
command = "ls {flags} {path}"
description = "List the contents of a directory."
parameter_type = "object"
[tool.list_dir.parameters]
type = "object"
required = ["path"]

[tool.list_dir.parameters.properties.path]
type = "string"
default = "."

[tool.list_dir.parameters.properties.flags]
type = "string"
default = "-la"


[tool.grep_search]
command = "grep -r --exclude-dir={.venv,node_modules} {pattern} {path}"
description = "Search for a pattern in files recursively."

[tool.grep_search.parameters]
type = "object"
required = ["pattern"]

[tool.grep_search.parameters.properties.pattern]
type = "string"
description = "The regex pattern to search for."

[tool.grep_search.parameters.properties.path]
type = "string"
description = "The directory path to search in."
default = "."

[command_presets]
# Command-line presets are invoked with "\" as the first token.
# Examples:
#   asky \daily "topic"
#   XMPP message: \daily "topic"
#
# Placeholders:
#   $1..$9 = positional arguments
#   $*     = all trailing arguments
#
# Remaining arguments that are not explicitly referenced are appended automatically.
daily = "--shortlist on Give me a concise daily briefing about $*"
research_local = "-r $1 Summarize the local corpus and answer: $*"

# -----------------------------------------------------------------------------
# Optional prompt/tool override knobs (examples, commented out by default).
# Copy these into your own ~/.config/asky/*.toml and adjust as needed.
# -----------------------------------------------------------------------------
#
# [prompts]
# research_retrieval_only_guidance = """A research corpus has already been preloaded.
# Prioritize query_research_memory + get_relevant_content.
# Do not discover new URLs unless the user explicitly asks."""
# interface_planner_system = """You are an asky interface planner.
# Return ONLY JSON with action_type, command_text, query_text.
# Emit command_text without /asky prefix."""
#
# [prompts.tool_overrides.web_search]
# description = "Search for candidate sources when local corpus is insufficient."
# system_prompt_guideline = "Use only when retrieval from preloaded sources is insufficient."
#
# [prompts.tool_overrides.get_url_content]
# description = "Fetch one or more HTTP(S) pages and extract main content."
# system_prompt_guideline = "Prefer batching multiple URLs in one call."
#
# [prompts.tool_overrides.get_relevant_content]
# description = "Retrieve top relevant chunks from cached/indexed pages."
# system_prompt_guideline = "Use with narrow sub-queries before any full-content request."
#
# [prompts.tool_overrides.save_finding]
# description = "Save source-backed research evidence for later recall."
# system_prompt_guideline = "Persist validated claims with source_url and tags."
#
# [prompts.tool_overrides.save_memory]
# description = "Save durable user preferences/facts (not research evidence)."
# system_prompt_guideline = "Only call for long-term user profile facts."
